name: "Ollama"
description: "Agent using local Ollama models (requires Ollama installed)"
id: "ollama"
default_model: "llama3.2"
requires:
  - ollama_url: "OLLAMA_API_URL"
  - ollama_model: "OLLAMA_MODEL"
files:
  agent.py: |
    """
    Your local Ollama agent.
    """
    from google.adk.agents import Agent
    from code_agent.agents.ollama.adk_integration import OllamaLlm

    # Create custom Ollama LLM for ADK
    ollama_llm = OllamaLlm(
        model="{ollama_model}",  # Use your preferred Ollama model
        base_url="{ollama_url}"  # Adjust if your Ollama server is on a different address
    )

    # Initialize agent with the custom LLM
    root_agent = Agent(
        model=ollama_llm,  # Use the OllamaLlm instance instead of model name string
        name='root_agent',
        description='A helpful assistant powered by a local Ollama model.',
        instruction='Answer user questions to the best of your knowledge.'
        # Note: To use Ollama, ensure your configuration is set up with:
        # default_provider: "ollama" in ~/.config/code-agent/config.yaml
        # or use --provider ollama when running commands
    )


    # If you encounter ADK integration issues, you can use the direct provider:
    """
    from code_agent.agents.ollama import OllamaDirectProvider

    # Initialize with your model and Ollama server URL
    ollama_provider = OllamaDirectProvider(
        model="{ollama_model}",
        base_url="{ollama_url}"
    )

    # Example usage:
    # response = ollama_provider.generate("What is your name?")
    # print(response)
    """
  __init__.py: |
    from . import agent
  .env: |
    # Ollama configuration
    # You need to run the CLI with --provider ollama or set default_provider: ollama in config.yaml
    # OLLAMA_API_URL={ollama_url}
    # OLLAMA_MODEL={ollama_model}
setup_instructions: |
  Your Ollama agent has been created.
  
  To use your agent with ADK integration:
    code-agent run "Your query here" {agent_folder} --provider ollama
    
  Alternatively, you can use the direct provider shown in the agent.py file
  for more reliable integration with your local Ollama models.
  
  Make sure Ollama is running on your machine with:
    ollama serve
    
  And that you've pulled the required model:
    ollama pull {ollama_model} 